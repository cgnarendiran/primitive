
<!DOCTYPE html>
<html lang="en">
  <head>
  <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
    <title>Sai Prabhakar</title>
    <!-- Bootstrap -->
    <link href="bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="bootstrap/css/other.css" rel="stylesheet">
    <!-- Enable fluid response -->
    <!-- <meta name="viewport" content="width=device-width, initial-scale=1.0"> -->
    <!-- <link href="assets/css/bootstrap-responsive.css" rel="stylesheet"> -->

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-56794354-1', 'auto');
  ga('send', 'pageview');

  </script>

    <script src="bootstrap/js/jquery-latest.js"></script>
    <script src="bootstrap/js/bootstrap.min.js"></script>
    <!-- Analytics Code -->
    <script type="text/javascript">
    </script>
  </head>
  <body data-spy="scroll" data-target=".sideNav">
    <div class="navbar navbar-fixed-top navbar-inverse">
      <div class="navbar-inner">
        <div class="container">
          <a class="brand" href="index.html">Sai Prabhakar</a>
          <ul class="nav" id="mainTabs">
            <li><a href="index.html"><i class="icon-home icon-white"></i> Home</a></li>
            <li class="active"><a href="projects.html"><i class="icon-cog icon-white"></i> Projects</a></li>
            <li><a href="files/Sai_Prabhakar_Resume.pdf" target="_blank"><i class="icon-list-alt icon-white"></i> Resume</a></li>
            <li><a href="courses.html"><i class="icon-book icon-white"></i> Academics</a></li>
            <li><a href="contact.html"><i class="icon-envelope icon-white"></i> Contact</a></li>
            <!-- <li><a href="#">Link</a></li> -->
            <li><a href="documents.html"><i class="material-icons">favorite</i> Documents</a></li>
          </ul>
        </div>
      </div>
    </div>
    <div class="container">
      <div class="row">
        <div class="span3 sideNav">
          <ul id="resumeSideNavBar" class="nav nav-pills nav-stacked affix">

            <li><h4> Ongoing</h4></li>
                <li><a href="#verb">Verbalization (Ongoing)</a></li>
                

            <li ><h4>Projects</h4></li>
                <li><a href="#ped">Pedestrian detection</a></li>
                <li><a href="#course1">Deep learning driving</a></li>
                <li><a href="#course2">Speaker recognition</a></li>
                <li><a href="#course3">Human Detection</a></li>
                <li><a href="#syn">Handwritting syn. and recog.</a></li>
                <li><a href="#mpc">Model Predictive Control</a></li>
                <li><a href="#road">Drivable Road area detection</a></li>
                <li><a href="#dead">Dead reckoning</a></li>
                <br>
                <br>
                
            <li ><h4>Technical work</h4></li>
                <li><a href="#robo">ABU Robocon</a></li>
                <li><a href="#rov">Shaastra Lunar Rover Challenge</a></li>
                <li><a href="#amada">Software Development</a></li>
          </ul>
        </div>

        <div class="span9">
          <!--div class="page-header">
            <center><h1>Research Projects</small></h1></center>
          </div-->
          
          
          
            
          <section id="verb">
            <h3>Verbalization: Describing Robot's Experience (Ongoing)</h3>
            <h5>Thesis work</h5><p> Guides: <a href="http://www.cs.cmu.edu/~mmv/" target="_blank">Prof. Manuela Veloso</a> and <a href="http://www.cs.cmu.edu/~srosenth/" target="_blank">Dr. Stephanie Rosenthal</a> (Sep'15 - Present)
            <br>
            
            <h5>Objective</h5>
            <p>
                Created a novel method-- Verbalization, to generate explanation for robot's experience and actions, based on the user's preference. Motivation for expressing robot's experience lies in the fact that this has been shown to improve the trustworthiness of the robots. This concept has been used to describe the path taken by the CoBot.  </p>
            <p>
                Future work involves extending the verbalization to describe interesting event that happened to the robots in vision and path experience.
            </p>
            
            <center><img src="img/c5-1.png" width="200" class="img-rounded"/>
            <p>CoBot waiting for elevator</p></center>
            
            <p>CoBot is a mobile service robot designed to traverse in corridors and elevator. Our CoBot robots follow a novel symbiotic autonomy, in which the robots are aware of their perceptual, physical, and reasoning limitations and pro-actively ask for help from humans, for example for object manipulation actions. CoBot robots have successfully performed variety of service tasks in our multi-building environment including accompanying people to meetings and delivering objects to offices due to its navigation and localization capabilities.</p>
            
            <h5>
            Publications:
            </h5>
            <p>
            <a href="files/16ijcai-verbalization.pdf" target="_blank">Verbalization: Narration of Autonomous Mobile Robot Experience</a>,
            Stephanie Rosenthal, Sai P. Selvaraj, and Manuela Veloso.
            In Proceedings of IJCAI'16, the 26th International Joint Conference on Artificial Intelligence, New York City, NY, July, 2016.
            </p>
            
            <p>
            <a href="files/16roman-verbalization.pdf" target="_blank">Dynamic Generation and Refinement of Robot Verbalization</a>,
            Vittorio Perera, Sai P. Selvaraj, Stephanie Rosenthal, and Manuela Veloso. 
            In Proceedings of RO-MAN'16, the IEEE International Symposium on Robot and Human Interactive Communication, Columbia University, NY, August, 2016.
            </p>
            
            
          </section>



        <section id="ped">
            <h3>Pedestrian detection using Fast-RCNN</h3>
            <h5> Internship: Delphi Automotive, Pittsburgh</a>  (May'16 - Aug'16)
            <br>
            <h5>Objective</h5>
            <p>
            Implemented Fast-RCNN and Scale-aware Fast-RCNN for pedestrian detection using AlexNet based architecture in Caffe. The network were trained using Caltech Pedestrian dataset and tested for real time performance by mounting in car.
            <p>
            Achieved a state-of-the-art miss rate of 7.2%, using Fast-RCNN, improved it to 5.2% using HDR images. Detection performance was testing using HDR images which offered slight improvement in detection performance.
            </p>
            
            <center><img src="img/ped_sample.png" width="700" />
            <p>Results of pedestrian detection on newly collected data</p></center>
            
            
        </section>


          <section id="course1">
            <h3>Direct perception based automation of car in racing game</h3>
             <h5>Course Project, CMU</h5>
            <h5>Objective</h5>
            <p>
              Today, there are two major paradigms for vision-based autonomous driving systems: mediated perception approaches that parse an entire scene to make a driving decision, and behavior reflex approaches that directly map an input image to a driving action by a regressor. A third paradigm: a direct perception based approach to estimate the affordance for driving was proposed <a href= "http://deepdriving.cs.princeton.edu/paper.pdf" target="_blank">recently</a>. Where the input image is mapped to a small number of key perception indicators that directly relate to the affordance of a road/traffic state for driving. The work uses AlexNet training from scratch for the mapping.</p>
              
              <p>This worked explored the possibility using transfer learning for learning the mapping. We used pre-trained Alexnet to map the input image to a set of key perception indicators. These indicators enables a simple controller to drive the car autonomously. We fine-tunned weights for all the convolution layers (layers 1-5) in the network and trained from scratch for all the fully connected layers (layers 6-8) to preserve the aspect ratio of the images obtained from the simulator.
            </p>
            <p>
                Pre-trained Alexnet produced similar result compared to previous work, which trained from scratch.
            </p>
            <p>More details can be found <a href="files/deep_driving.pdf" target="_blank">here</a></p>
            <p>Codes available  <a href="https://github.com/saiprabhakar/DeepDriving" target="_blank">here</a></p>
            
<center><iframe width="560" height="315" src="https://www.youtube.com/embed/Ffyi9W23Fl0" frameborder="0" allowfullscreen></iframe></center>
             
<center><br><em>Performance of the direct perception approach on TORCS racing game </em><br></center>

          </section>

        <section id="course2">
            <h3>LSTM based speaker recognition</h3>
            <h5>Course Project, CMU</h5>
            <h5>Objective</h5>

            <p>
                There has been significant improvement in the recognition accuracy due to the recent resurgence of deep neural networks. In this work we built a LSTM based speaker recognition system on a dataset collected from Cousera lectures-- text independent and noisy dataset. On which a state-of-the-art testing time accuracy of 93% was obtained.
            </p>
            <p>
                The final network after experimenting was 2 layer deep with 25 LSTM units each. Performance in text independent dataset, were equivalent to state-of-the-art LSTM (Bidirectional-LSTM) based results.
            </p>
            <p>More details can be found <a href="files/lstm_speaker.pdf" target="_blank">here</a></p>
            <p> Codes available  <a href="https://github.com/saiprabhakar/rnn" target="_blank">here</a></p>
            
          </section>

        <section id="course3">
            <h3>Fast Human Detection using only depth image</h3>
            <h5>Course Project, CMU</h5><p>
            <h5>Objective</h5>
            <p>
              Mobile robots possess limited computational resources which are shared among various processes required for autonomous operation, so the human detector algorithm must have a small computational footprint. The detections has to be robust to variation in pose, occlusion and lighting variation. In this work we have implemented an algorithm for fast and robust human detection in indoor environment using only depth image.
            </p>
            <p>
                The algorithm involves multiple graph segmentation, heuristic based region filter to obtain candidates for potential human. Finally the candidates are classified using SVM based classification.
            </p>
            
            <p>More details can be <a href="files/human_detect.pdf" target="_blank">here</a></p>
            
            <center><img src="img/human_seg1.png" width="300" class="img-rounded"/>
            <p>Results of segmentation algorithm</p></center>
            
          </section>


<!--
          <hr class="separator" />

          <section id="recog">
            <h3>Developing Character recognition Engine</h3>
            <h5>Guides</h5><p>Prof. Srinivasa Chakravarthy.V, Computational Neuroscience Lab, IIT Madras (Aug'14 - Apr'15)<br>
            <h5>Abstract</h5>
            <p>
              Develop engines for Online Character Recognition. We developed recognition engine using SVM with SVTorch. Using shape features extracted from the Strokes and achieved 98.3% accuracy. Currently exploring the possibility of using Neural Networks for the same. Presently with Deep Belief Neural Network, 88.2% accuracy was achieved (Ongoing).
            </p>
            <p>Currently I have also joined as a Student Project Representative with Prof. Chakaravarthy to develope sofware for the recognition engine.</p>
          </section>

          <hr class="separator" />
-->


          <section id="syn">
            <h3>Online Character Synthesis and Recognition</h3>
            <h5>Guide:</h5><p>Prof. Srinivasa Chakravarthy.V, Computational Neuroscience Lab, IIT Madras (Aug'14 - Apr'15)</p>
            <h5>Abstract</h5>
            <p>
              Create a novel method to synthesis online characters, irrespective of the language. We creating a general Structure which can accommodate all the characteristics and variation of different languages and modelled it to resemble human writing. Intended for training set expansion for developing online character recognition engines.
            </p>
            <p>
            Implemented Deep Neural Network (DNN) using Auto-Encoders, on a very sparse training set, and obtained overall accuracy of 88.2%. Worked as student project associate on software development for recognition engine.
            </p>
          </section>
          
          <hr class="separator" />
          
          <section id="mpc">
            <h3>Improved Model Predictive Control Algorithm for UAVs</h3>
            <h5>Guide</h5>
            <p>Prof. Alejandro Ramirez-Serrano, AR2S Lab, Univ. of Calgary, Canada (May'14 - Aug'14)</p>
            <h5>Abstract</h5>
Objective is to decreased the Computational time of a existing MPC navigation algorithm, which takes into account the shape and mass of the UAV through confined 3D environment. Decreased the Computational time by 6.5 times.<br>
For increasing speed while maintaining accuracy separate methods were developed, for the cases low SNR and high SNR range sensor data.  For low SNR, geometrical approximation by RANSAC, and for high SNR, ecient data structure were used to reasonably approximate the cost function faster.<br><br>
          <center><img src="img/r.gif"> <br></center>
          <center><br><em>The MPC Navigation Algorithm at work</em><br></center>
          </section>
          <hr class="separator" />
          

          <section id="road">
            <h3>Drivable Road area detection </h3>
                <h5>Guide</h5>
                <p>Prof. P.V.Maninvannan, IIT Madras (Oct'13 - May'14)</p>
                <h5>Abstract</h5>
                <p>
                Surveyed literature and implemented different methods in supervised and unsupervised learning for segmenting drivable road area. In supervised learning, I developed a modified version of Eigenface method. In unsupervised learning, I explored different features like Colour space (RGB, HSB, ATan) and Texture (Haralick and Laws energy measure) with GMM clustering.</p>
<!--            <div class="row">-->
<!--              <div class="span4">-->
<!--                  <img src="img/Force_Sensor.jpg" />-->
<!--              </div>-->
<!--              <div class="span5"><img src="img/meter.png" class="img-rounded"/></div>-->
<!--            </div>-->
<!--            <em>Entry to the GE Edison Challenge 2011</em>-->

            <div class="row">
              <div class="span3"><img src="img/13_o.jpg" class="img-rounded"/></div>
              <div class="span3"><img src="img/19_o.jpg" class="img-rounded"/></div>
              <div class="span3"><img src="img/8_o.jpg" class="img-rounded"/></div>              
            </div>
        <em><p><center>Original Image</center></p></em>
            <div class="row">
              <div class="span3"><img src="img/13.jpg" class="img-rounded"/></div>
              <div class="span3"><img src="img/19.jpg" class="img-rounded"/></div>
              <div class="span3"><img src="img/8.jpg" class="img-rounded"/></div>              
            </div>
        <em><p><center>After Road Detection</center></p></em>

          </section>
          <hr class="separator" />
          <section id="dead">
            <h3>Algorithm development for Dead reckoning</h3>
            <h5>Guide</h5>
            <p>Center For Innovation, IIT Madras (May'12 - Jul'12)</p>
            <h5>Abstract</h5>
            <p>Designed and developed a robot of 0.6m.x0.6m capable of following specified path accurately using using Dead Reckoning algorithm with two wheel encoder, while halting in the point at specific check points with was achieved within a distance of 0.03m, while carrying mechanism of varying centre of mass. Additionally I also designed the circuit and implemented PID control system for it.</p>
<!--            <table class="table table-bordered table-hover">-->
<!--              <tr><td>-->
<!--                Proof of concept can be viewed <a href="files/Smart-Water-System.pdf" target="_blank">here. <i class="icon-download"></i></a>-->
<!--              </td></tr>-->
<!--            </table>-->
<!--            <h5>Control systems</h5>-->
<!--            <p>Control systems for a smart water management system were developed and simulated in MATLAB Simulink.</p>-->
<!--            <div class="row">-->
<!--              <a href="img/mlwss-control.png" target="_blank"><img src="img/mlwss-control.png" /></a>-->
<!--              <a href="img/mlwss-sub.png" target="_blank"><img src="img/mlwss-sub.png" /></a>-->
<!--            </div>-->
<!--            <p><em>Entry to the GE Edison Challenge 2011</em>-->
          </section>
          <hr class="separator" />
          
          <!--section id="Tech">
            <center><h1>Technical Projects</h1></center>
            <hr class="separator" />
          </section-->



          <section id="robo">
            <h3>International Robotics Contest, ABU Robocon</h3>
                      
            <p>Represented IIT Madras in, ABU Robocon for the year 2013.<p>

<p>Problem Statement  involves build a Autonomous and a Manual Robot of 0.6mx0.6mx1.0m, both capable of navigating through a Known terrain, pick and place objects and transfer object between them. And throw a object and make it land in a disc of diameter 60cm, which is approximately 1.5m high and 5m away from the robot.</p>

<p>Implemented Dead Reckoning algorithm I previously developed, to navigate using two wheel encoder. As a team we also worked in automating the task involved and developed strategies to communicate with other robot while performing coordinated tasks.</p>

    <p>Worked with a team of 24 students. {Won Fastest Job Completion Award} for the year 2013 in the National level</p>

<p>Our team Won Fastest Job Completion Award for the year 2013 in national level, among over 80 teams.</p>
<!--            <div class="row">-->
<!--              <div class="span4"><img src="img/Mechanism1.gif" alt="Mechanism1" class="img-rounded" /></div>-->
<!--              <div class="span4"><img src="img/Mechanism2.gif" alt="Mechanism2" class="img-rounded" /></div>-->
<!--            </div>-->

 <center><iframe width="420" height="315" src="//www.youtube.com/embed/M6jNqmd_Jek" frameborder="0" allowfullscreen></iframe></center>
<center><br><em>This is a video of our bot performing at the National Round in Pune, India</em><br></center>            
          </section>

          <hr class="separator" />

          <section id="rov">
          <h3>National Level, Shaastra Lunar Rover Challenge</h3>
          <div class="row">
          <div class="span4">
             <p>Objective is to develope a small robot of 0.25m.x0.25m capable of performing the tasks of a rover like communication, live video transmission, collision avoidance and detecting appropriate fags, in an artificial lunar surface.</p>
            <p>Placed Second in National level among over 90 teams.</p>
          </div>
<!--<img src="img/19_o.jpg" class="img-rounded"/></div>-->
          <div class="span4"><img src="img/lunar.jpg" class="img-rounded"/>
        <center><p>Our team and Rover</p></center>
            </div>
        </div>

        <hr class="separator" />

          
          <section id="amada">
            <h3>Software Development for CAD/CAM software</h3>
              <h5>Amda Soft India [Dec'13' - Jan'14]</h5>
              <h5>Guide</h5>
              <p>Mr.Elisha Madhu Kumar Karyamsetty, CTO</p>
              <h5>Astract</h5>
              <p>Objective is to Render fonts from different languages by converting them to vector data using B-splines information from font TTF file, for using it in CAD/CAM software. Used Visual Studio C++. Learnt a lot about efficient, clean programming and memory management while I worked with various data structure and templates.</p><br><br>
    
          </section>
          <hr class="separator" />
          <br>
        </div>
      </div>
    </div>
  </body>
  <!-- // $('img').click(function(){window.open($(this).attr('src'));console.log($(this));});
  // $('img').hover(function(){$(this).css('cursor','pointer');},function(){$(this).css('cursor','auto');}); -->
</html>
